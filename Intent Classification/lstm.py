# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17knDaa7agc6FdletRDZJTB87IjTAFXsZ
"""

import pandas as pd
from nltk.tokenize import word_tokenize
import string
import re
import nltk
nltk.download('punkt')

# Load the dataset
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

# Define a function to normalize and tokenize the text
def normalize_and_tokenize(text):
    # Remove special characters and punctuation symbols
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    # Convert text to lowercase
    text = text.lower()
    # Tokenize the text
    tokens = word_tokenize(text)
    return tokens

# Apply the function to the 'text' column of the train and test datasets
train_df['text'] = train_df['text'].apply(normalize_and_tokenize)
test_df['text'] = test_df['text'].apply(normalize_and_tokenize)

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip glove.6B.300d.txt

import numpy as np

# Load the pre-trained GloVe embeddings
embeddings_index = {}
with open('glove.6B.300d.txt', 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

# Define a function to get the embedding for a given word
def get_embedding(word):
    embedding = embeddings_index.get(word)
    if embedding is not None:
        return embedding
    else:
        return np.zeros(300)

# Define a function to get the embeddings for a given text
def get_text_embeddings(text):
    embeddings = []
    for word in text:
        embedding = get_embedding(word)
        embeddings.append(embedding)
    return embeddings

# Apply the function to the 'text' column of the train and test datasets
train_df['text_embeddings'] = train_df['text'].apply(get_text_embeddings)
test_df['text_embeddings'] = test_df['text'].apply(get_text_embeddings)

from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.utils import to_categorical
!pip install keras_preprocessing
from keras_preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt

# Set the number of classes
num_classes = 6

# Convert the 'label-coarse' column to one-hot encoded labels
y_train = to_categorical(train_df['label-coarse'], num_classes=num_classes)
y_test = to_categorical(test_df['label-coarse'], num_classes=num_classes)

# Define the maximum sequence length (you can choose a different value)
max_seq_length = 100

# Pad or truncate the 'text_embeddings' sequences to have the same length
X_train = pad_sequences(train_df['text_embeddings'], maxlen=max_seq_length, dtype='float32')
X_test = pad_sequences(test_df['text_embeddings'], maxlen=max_seq_length, dtype='float32')

# Define the input shape
input_shape = (max_seq_length, 300)

# Create the model
model = Sequential()
model.add(LSTM(25, input_shape=input_shape))
model.add(Dense(num_classes, activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model and save the training history
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)

# Plot the train and test accuracy over epochs
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot the train and test loss over epochs
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

from sklearn.metrics import confusion_matrix, classification_report

# Get the predictions for the test dataset
y_pred = model.predict(X_test)
y_pred_classes = y_pred.argmax(axis=1)

# Convert the one-hot encoded test labels back to class labels
y_test_classes = y_test.argmax(axis=1)

# Compute the confusion matrix
cm = confusion_matrix(y_test_classes, y_pred_classes)

# Compute the classification report
report = classification_report(y_test_classes, y_pred_classes)

# Evaluate the model on the test dataset
_, test_acc = model.evaluate(X_test, y_test, verbose=0)

# Print the confusion matrix and classification report
print(f'Test Accuracy: {test_acc:.2f}')
print('Confusion Matrix:')
print(cm)
print('Classification Report:')
print(report)

from keras.models import Sequential
from keras.layers import LSTM, Dense
from keras.utils import to_categorical
!pip install keras_preprocessing
from keras_preprocessing.sequence import pad_sequences
import matplotlib.pyplot as plt

# Set the number of classes
num_classes = 6

# Convert the 'label-coarse' column to one-hot encoded labels
y_train = to_categorical(train_df['label-coarse'], num_classes=num_classes)
y_test = to_categorical(test_df['label-coarse'], num_classes=num_classes)

# Define the maximum sequence length (you can choose a different value)
max_seq_length = 100

# Pad or truncate the 'text_embeddings' sequences to have the same length
X_train = pad_sequences(train_df['text_embeddings'], maxlen=max_seq_length, dtype='float32')
X_test = pad_sequences(test_df['text_embeddings'], maxlen=max_seq_length, dtype='float32')

# Define the input shape
input_shape = (max_seq_length, 300)

# Create the model
model = Sequential()
model.add(LSTM(100, input_shape=input_shape))
model.add(Dense(num_classes, activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model and save the training history
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32)

# Plot the train and test accuracy over epochs
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot the train and test loss over epochs
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

from sklearn.metrics import confusion_matrix, classification_report

# Get the predictions for the test dataset
y_pred = model.predict(X_test)
y_pred_classes = y_pred.argmax(axis=1)

# Convert the one-hot encoded test labels back to class labels
y_test_classes = y_test.argmax(axis=1)

# Compute the confusion matrix
cm = confusion_matrix(y_test_classes, y_pred_classes)

# Compute the classification report
report = classification_report(y_test_classes, y_pred_classes)

# Evaluate the model on the test dataset
_, test_acc = model.evaluate(X_test, y_test, verbose=0)

# Print the confusion matrix and classification report
print(f'Test Accuracy: {test_acc:.2f}')
print('Confusion Matrix:')
print(cm)
print('Classification Report:')
print(report)

from keras.models import Model
from keras.layers import Input, LSTM, Dense
from keras.utils import to_categorical

# Set the number of classes
num_coarse_classes = 6
num_fine_classes = 50

# Convert the 'label-coarse' and 'label-fine' columns to one-hot encoded labels
y_coarse_train = to_categorical(train_df['label-coarse'], num_classes=num_coarse_classes)
y_coarse_test = to_categorical(test_df['label-coarse'], num_classes=num_coarse_classes)
y_fine_train = to_categorical(train_df['label-fine'], num_classes=num_fine_classes)
y_fine_test = to_categorical(test_df['label-fine'], num_classes=num_fine_classes)

# Define the maximum sequence length
max_seq_length = 100

# Add a <PAD> token at the end of each text sequence
train_df['padded_text'] = train_df['text'].apply(lambda x: x + ['<PAD>'])
test_df['padded_text'] = test_df['text'].apply(lambda x: x + ['<PAD>'])

# Get the embeddings for the padded text sequences
X_train = train_df['padded_text'].apply(get_text_embeddings)
X_test = test_df['padded_text'].apply(get_text_embeddings)

# Pad or truncate the 'text_embeddings' sequences to have the same length
X_train = pad_sequences(X_train, maxlen=max_seq_length + 1, dtype='float32')
X_test = pad_sequences(X_test, maxlen=max_seq_length + 1, dtype='float32')

# Define the input shape
input_shape = (max_seq_length + 1, 300)

# Create the input layer
inputs = Input(shape=input_shape)

# Create the LSTM layer
lstm_outputs = LSTM(25, return_sequences=True)(inputs)

# Create the coarse and fine output layers
coarse_outputs = Dense(num_coarse_classes, activation='softmax')(lstm_outputs[:, -2, :])
fine_outputs = Dense(num_fine_classes, activation='softmax')(lstm_outputs[:, -1, :])

# Create the model
model = Model(inputs=inputs, outputs=[coarse_outputs, fine_outputs])

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, [y_coarse_train, y_fine_train], validation_data=(X_test, [y_coarse_test, y_fine_test]), epochs=50, batch_size=32)

import matplotlib.pyplot as plt

# Plot the train and test accuracy over epochs
plt.plot(history.history['dense_5_accuracy'])
plt.plot(history.history['val_dense_5_accuracy'])
plt.title('Model Accuracy 5')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history.history['dense_6_accuracy'])
plt.plot(history.history['val_dense_6_accuracy'])
plt.title('Model Accuracy 6')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot the train and test loss over epochs
plt.plot(history.history['dense_5_loss'])
plt.plot(history.history['val_dense_5_loss'])
plt.title('Model Loss 5')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history.history['dense_6_loss'])
plt.plot(history.history['val_dense_6_loss'])
plt.title('Model Loss 6')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

from keras.models import Model
from keras.layers import Input, LSTM, Dense
from keras.utils import to_categorical

# Set the number of classes
num_coarse_classes = 6
num_fine_classes = 50

# Convert the 'label-coarse' and 'label-fine' columns to one-hot encoded labels
y_coarse_train = to_categorical(train_df['label-coarse'], num_classes=num_coarse_classes)
y_coarse_test = to_categorical(test_df['label-coarse'], num_classes=num_coarse_classes)
y_fine_train = to_categorical(train_df['label-fine'], num_classes=num_fine_classes)
y_fine_test = to_categorical(test_df['label-fine'], num_classes=num_fine_classes)

# Define the maximum sequence length
max_seq_length = 100

# Add a <PAD> token at the end of each text sequence
train_df['padded_text'] = train_df['text'].apply(lambda x: x + ['<PAD>'])
test_df['padded_text'] = test_df['text'].apply(lambda x: x + ['<PAD>'])

# Get the embeddings for the padded text sequences
X_train = train_df['padded_text'].apply(get_text_embeddings)
X_test = test_df['padded_text'].apply(get_text_embeddings)

# Pad or truncate the 'text_embeddings' sequences to have the same length
X_train = pad_sequences(X_train, maxlen=max_seq_length + 1, dtype='float32')
X_test = pad_sequences(X_test, maxlen=max_seq_length + 1, dtype='float32')

# Define the input shape
input_shape = (max_seq_length + 1, 300)

# Create the input layer
inputs = Input(shape=input_shape)

# Create the LSTM layer
lstm_outputs = LSTM(100, return_sequences=True)(inputs)

# Create the coarse and fine output layers
coarse_outputs = Dense(num_coarse_classes, activation='softmax')(lstm_outputs[:, -2, :])
fine_outputs = Dense(num_fine_classes, activation='softmax')(lstm_outputs[:, -1, :])

# Create the model
model = Model(inputs=inputs, outputs=[coarse_outputs, fine_outputs])

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, [y_coarse_train, y_fine_train], validation_data=(X_test, [y_coarse_test, y_fine_test]), epochs=50, batch_size=32)

import matplotlib.pyplot as plt

# Plot the train and test accuracy over epochs
plt.plot(history.history['dense_9_accuracy'])
plt.plot(history.history['val_dense_9_accuracy'])
plt.title('Model Accuracy 9')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history.history['dense_10_accuracy'])
plt.plot(history.history['val_dense_10_accuracy'])
plt.title('Model Accuracy 10')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# Plot the train and test loss over epochs
plt.plot(history.history['dense_9_loss'])
plt.plot(history.history['val_dense_9_loss'])
plt.title('Model Loss 9')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

plt.plot(history.history['dense_10_loss'])
plt.plot(history.history['val_dense_10_loss'])
plt.title('Model Loss 10')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()